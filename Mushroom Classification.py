# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oWmrfhBK9vcmJx723-OqLm3GGWx9gMmm
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LinearRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, ConfusionMatrixDisplay
from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import fetch_openml

# Fetch dataset Mushroom dari OpenML
mushroom = fetch_openml(name='mushroom', version=1, as_frame=True)

# Memisahkan fitur dan target
X = mushroom.data
y = mushroom.target

# Metadata
print(mushroom.DESCR)

# Variable Information

data_df = pd.concat([X, y], axis=1)
data_df.head()

data_df.describe(include='all')

X.info()
print ("==========================================================================================")
y.info()

(X.isnull().sum())

(X.)

class_counts = data_df['class'].value_counts()
data_df['class'].value_counts()



sns.countplot(x=y, palette='Set2')
plt.title('Distribution Class')
plt.xlabel('Class')
plt.ylabel('Jumlah Sampel')
plt.xticks(ticks=[0,1], labels=['Edible (e)', 'Poisonous (p)'])
plt.show()

plt.figure(figsize=(10,6))
sns.countplot(x='odor', hue=y, data=X, palette='Set1')
plt.title('Odor berdasarkan Class')
plt.xlabel('Odor')
plt.ylabel('Jumlah Sampel')
plt.legend(title='Kelas', labels=['Edible (e)', 'Poisonous (p)'])
plt.show()

le = LabelEncoder()
X_encoded = X.copy()

for column in X_encoded.columns:
    X_encoded[column] = le.fit_transform(X_encoded[column])

X_encoded.head()



sns.countplot(x='stalk-root', data=X)
plt.title('')
plt.show()

stalk_root_mode = X['stalk-root'].mode()[0]
print(f"Modus stalk-root: {stalk_root_mode}")
X['stalk-root'].fillna(stalk_root_mode, inplace=True)
print(f"Jumlah missing values setelah imputasi: {X['stalk-root'].isnull().sum()}")

le = LabelEncoder()
categorical_columns = data_df.select_dtypes(include=['object', 'category']).columns

for col in categorical_columns:
    data_df[col] = le.fit_transform(data_df[col])

data_df_encoded = data_df.copy()

X = data_df_encoded.drop('class', axis=1)
y = data_df_encoded['class']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

X_scaled.head()

X = data_df.drop('class', axis=1)
y = data_df['class']

X_encoded = pd.get_dummies(X, drop_first=True)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)
X_scaled = pd.DataFrame(X_scaled, columns=X_encoded.columns)

le = LabelEncoder()
y_final = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_final, test_size=0.2, random_state=42, stratify=y_final
)

print(f'Jumlah data latih: {X_train.shape[0]}')
print(f'Jumlah data uji: {X_test.shape[0]}')

y_train_series = pd.Series(y_train)
y_test_series = pd.Series(y_test)

print("\nDistribusi Class - Training Set:")
print(y_train_series.value_counts(normalize=True))

print("\nDistribusi Class - Testing Set:")
print(y_test_series.value_counts(normalize=True))

y_train_series = pd.Series(y_train)
y_test_series = pd.Series(y_test)

print("Jumlah Class di training set:")
print(y_train_series.value_counts())

print("\nJumlah Class di testing set:")
print(y_test_series.value_counts())

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

sns.countplot(x=y_train, ax=axes[0], color='blue')
axes[0].set_title('Distribusi Class - Training Set')

sns.countplot(x=y_test, ax=axes[1], color='red')
axes[1].set_title('Distribusi Class - Testing Set')

plt.tight_layout()
plt.show()

svm_model = SVC(kernel='linear', probability=True, random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

print("K-NN Classification Report:")
print(classification_report(y_test, y_pred_knn))

linreg_model = LinearRegression()

linreg_model.fit(X_train, y_train)
y_pred_linreg = linreg_model.predict(X_test)
y_pred_linreg_class = (y_pred_linreg >= 0.5).astype(int)
print("Linear Regression Classification Report:")
print(classification_report(y_test, y_pred_linreg_class))

mse = mean_squared_error(y_test, y_pred_linreg)
mae = mean_absolute_error(y_test, y_pred_linreg)
r2 = r2_score(y_test, y_pred_linreg)

X = data_df_encoded.drop('class', axis=1)
y = data_df_encoded['class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

svm_model = SVC()
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

cm_svm = confusion_matrix(y_test, y_pred_svm)
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=['Edible', 'Poisonous'])
disp_svm.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - SVM')
plt.show()

knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

cm_knn = confusion_matrix(y_test, y_pred_knn)
disp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=['Edible', 'Poisonous'])
disp_knn.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - K-NN')
plt.show()

linreg_model = LinearRegression()
linreg_model.fit(X_train, y_train)
y_pred_linreg = linreg_model.predict(X_test)
y_pred_linreg_class = (y_pred_linreg >= 0.5).astype(int)

cm_linreg = confusion_matrix(y_test, y_pred_linreg_class)
disp_linreg = ConfusionMatrixDisplay(confusion_matrix=cm_linreg, display_labels=['Edible', 'Poisonous'])
disp_linreg.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Linear Regression')
plt.show()

accuracy_svm = accuracy_score(y_test, y_pred_svm)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
accuracy_linreg = accuracy_score(y_test, y_pred_linreg_class)

print(f'Akurasi SVM: {accuracy_svm:.4f}')
print(f'Akurasi K-NN: {accuracy_knn:.4f}')
print(f'Akurasi Linear Regression: {accuracy_linreg:.4f}')

models = ['SVM', 'K-NN', 'Linear Regression']
accuracies = [accuracy_svm, accuracy_knn, accuracy_linreg]

for acc in accuracies:
    if not (0 <= acc <= 1):
        raise ValueError("Nilai akurasi harus berada dalam rentang 0 hingga 1.")

plt.figure(figsize=(10, 7))
sns.barplot(x=models, y=accuracies, palette='viridis')

y_max = max(accuracies) + 0.01
y_max = min(y_max, 1.5)
plt.ylim(0.90, y_max)

plt.title('Perbandingan Akurasi Model', fontsize=16)
plt.xlabel('Model', fontsize=14)
plt.ylabel('Akurasi', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)

for i, v in enumerate(accuracies):
    plt.text(i, v + 0.005, f"{v:.4f}", ha='center', va='bottom', fontweight='bold', fontsize=12)

plt.tight_layout()
plt.show()

y_prob_svm = svm_model.decision_function(X_test)
y_prob_knn = knn_model.predict_proba(X_test)[:,1]
y_prob_linreg = linreg_model.predict(X_test)

fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm)
roc_auc_svm = auc(fpr_svm, tpr_svm)

fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_knn)
roc_auc_knn = auc(fpr_knn, tpr_knn)

fpr_linreg, tpr_linreg, _ = roc_curve(y_test, y_prob_linreg)
roc_auc_linreg = auc(fpr_linreg, tpr_linreg)

plt.figure(figsize=(8,6))
plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_auc_svm:.2f})')
plt.plot(fpr_knn, tpr_knn, label=f'K-NN (AUC = {roc_auc_knn:.2f})')
plt.plot(fpr_linreg, tpr_linreg, label=f'Linear Regression (AUC = {roc_auc_linreg:.2f})')
plt.plot([0,1], [0,1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Model Comparison')
plt.legend(loc='lower right')
plt.show()

mse_linreg = mean_squared_error(y_test, y_pred_linreg)
mae_linreg = mean_absolute_error(y_test, y_pred_linreg)
r2_linreg = r2_score(y_test, y_pred_linreg)

print(f'Mean Squared Error (Linear Regression): {mse_linreg:.4f}')
print(f'Mean Absolute Error (Linear Regression): {mae_linreg:.4f}')
print(f'R2 Score (Linear Regression): {r2_linreg:.4f}')

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

epochs = 20

history = model.fit(
    X_train, y_train,
    epochs=epochs,
    validation_data=(X_test, y_test),
    callbacks=[early_stop],
    verbose=1
)

plt.figure(figsize=(12,6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

plt.figure(figsize=(12,6))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()